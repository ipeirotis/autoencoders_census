{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis/autoencoders_census/blob/main/Autoencoder_YRBSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7bkPX1yTObl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_url = \"https://github.com/ipeirotis/autoencoders_census/raw/main/sadc_2017only_national_full.csv\"\n",
        "df = pd.read_csv(dataset_url)\n",
        "print(df.head().to_markdown())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XCzCzSPQWaL"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = ['sitecode', 'sitename', 'sitetype', 'sitetypenum', 'year', 'survyear', 'record']\n",
        "df = df.drop(columns = columns_to_drop)\n",
        "df1 = df.iloc[:, :100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jcx7nyTXPyvm"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETIRJ_UcNPzI"
      },
      "outputs": [],
      "source": [
        "lst = [216, 232, 242, 245, 247, 249, 251, 252, 253, 256]\n",
        "df1 = pd.concat([df1, df.iloc[:, lst]], axis=1)\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlTg_odJQhCt"
      },
      "outputs": [],
      "source": [
        "dict1 = {\n",
        "        'age': 'age',\n",
        "        'sex': 'sex', 'grade':'grade','race4':'Hispanic or Latino','race7':'race',\n",
        "       'qnobese':'obese','qnowt':'overweight','q67':'sexual identity','q66':'sex/sexual contacts',\n",
        "       'sexid':'sexid','sexid2':'sexid2','sexpart':'sexpart','sexpart2':'sexpart2','q8':'seat belt use','q9':'riding with a drinking driver',\n",
        "       'q10':'drinking and driving','q11':'texting and driving','q12':'weapon carrying','q13':'weapon carrying at school',\n",
        "       'q14':'gun carrying past 12 mos','q15':'safety concerns at school','q16':'threatened at school','q17':'physical fighting',\n",
        "       'q18':'physical fighting at school','q19':'forced sexual intercourse','q20':'sexual violence','q21':'sexual dating violence',\n",
        "       'q22':'physical dating violence','q23':'bullying at school','q24':'electronic bullying','q25':'sad or hopeless',\n",
        "       'q26':'considered suicide','q27':'made a suicide plan','q28':'attempted suicide','q29':'injurious suicide attempt',\n",
        "       'q30':'ever cigarette use','q31':'initation of cigarette smoking','q32':'current cigarette use','q33':'smoking amounts per day',\n",
        "       'q34':'electronic vapor product use','q35':'current electronic vapor product use','q36':'EVP from store','q37':'current smokeless tobacco use',\n",
        "       'q38':'current cigar use','q39':'all tobacco product cessation','q40':'ever alcohol use','q41':'initiation of alcohol use',\n",
        "       'q42':'current alcohol use','q43':'source of alcohol','q44':'current binge drinking','q45':'largest number of drinks',\n",
        "       'q46':'ever marijuana use','q47':'initiation of marijuana use','q48':'current marijuana use','q49':'ever cocaine use',\n",
        "       'q50':'ever inhalant use','q51':'ever heroin use','q52':'ever methamphetamine use','q53':'ever ecstasy use',\n",
        "       'q54':'ever synthetic marijuana use','q55':'ever steroid use','q56':'ever prescription pain medicine use','q57':'illegal injected drug use',\n",
        "       'q58':'illegal drugs at school','q59':'ever sexual intercourse','q60':'first sex intercourse','q61':'multiple sex partners',\n",
        "       'q62':'current sexual activity','q63':'alcohol/drugs at sex','q64':'condom use','q65':'birth control pill use',\n",
        "       'q68':'perception of weight','q69':'weight loss','q70':'fruit juice drinking','q71':'fruit eating','q72':'green salad eating',\n",
        "       'q73':'potato eating','q74':'carrot eating','q75':'other vegetable eating','q76':'soda drinking','q77':'milk drinking',\n",
        "       'q78':'breakfast eating','q79':'physical activity','q80':'television watching','q81':'computer not school work use',\n",
        "        'q82':'PE attendance','q83':'sports team participation','q84':'concussion in last 12 mos','q85':'HIV testing','q86':'oral health care',\n",
        "       'q87':'asthma','q88':'sleep on school night','q89':'grades in school', 'qdrivemarijuana':'drive when using marijuana',\n",
        "       'qhallucdrug':'ever used LSD', 'qsportsdrink':'sports drinks','qwater':'plain water','qfoodallergy':'food allergies',\n",
        "        'qmusclestrength':'muscle stregthening','qindoortanning':'indoor tanning','qsunburn':'sunburn','qconcentrating':'difficulty concentrating',\n",
        "        'qspeakenglish':'how well speak English'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEUzA7-6REt4"
      },
      "outputs": [],
      "source": [
        "df1.rename(columns=dict1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oYuj_sWOs7n"
      },
      "outputs": [],
      "source": [
        "df1.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uleaSWAROzBa"
      },
      "outputs": [],
      "source": [
        "missing_percentages = df1.isnull().mean() * 100\n",
        "columns_with_missing_gt_25 = missing_percentages[missing_percentages > 25].index\n",
        "\n",
        "# Select the columns with missing values > 25%\n",
        "selected_columns = df1[columns_with_missing_gt_25]\n",
        "\n",
        "# Print the selected columns\n",
        "print(selected_columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over each variable in the dataframe\n",
        "for column in selected_columns:\n",
        "  # create a new column name by appending\n",
        "  missing_dummy_column = f'{column}_missing_dummy'\n",
        "  df1[missing_dummy_column] = df1[column].isnull().astype(int)\n",
        "\n",
        "# Display the updated dataframe\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "9DHNcgt8m_cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_num = df1[[\"weight\",\"stratum\",\"PSU\",\"stheight\",\"stweight\",\"bmi\",\"bmipct\",\"obese\",\"overweight\"]]"
      ],
      "metadata": {
        "id": "FX0HxEDkLAM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cat = df1.copy()\n",
        "data_cat = data_cat.drop(data_num.columns, axis = 1)\n",
        "data_cat.head()"
      ],
      "metadata": {
        "id": "k19THnfAMsZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scale on numerical columns--brings the value of each feature into the range of 0 to 1\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "def scaleNum(df_num, cols):\n",
        "    for col in cols:\n",
        "        df_num[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(data_num[col])),columns=[col])\n",
        "    return df_num\n",
        "data_normal_num = scaleNum(data_num,data_num.columns)"
      ],
      "metadata": {
        "id": "I33muwpSM4q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_normal_num.head()"
      ],
      "metadata": {
        "id": "nyvYEhL8Nk8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label encoding on categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "def scaleCat(df_cat, cols):\n",
        "    for col in cols:\n",
        "        df_cat[col] = pd.DataFrame(label_encoder.fit_transform(pd.DataFrame(data_cat[col])),columns=[col])\n",
        "    return df_cat\n",
        "data_normal_cat = scaleCat(data_cat,data_cat.columns)"
      ],
      "metadata": {
        "id": "7j_eGBEONVZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_normal_cat.head()"
      ],
      "metadata": {
        "id": "e-gA3hQ3NhiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge numerical columns and categorical columns based on their indices\n",
        "data = data_normal_num.merge(data_normal_cat, left_index = True, right_index = True)"
      ],
      "metadata": {
        "id": "a5jgD1pdNrIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_Oe8yvPm7Qv"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnFGYs0QRhdO"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "# the dimensionality of a latent space in an autoencoder\n",
        "latent_dimension = 1\n",
        "batch_size = 20\n",
        "# the number of nuerons in a hidden layer\n",
        "hidden_nodes = 16\n",
        "\n",
        "# create the input layer for the encoder\n",
        "input_encoder = Input(shape=(123,), name=\"Input_Encoder\")\n",
        "# apply batch normalization to the encoder input layer\n",
        "batch_normalize1 = BatchNormalization()(input_encoder)\n",
        "# create a hidden layer in the encoder\n",
        "hidden_layer = Dense(hidden_nodes, activation=\"relu\", name=\"Hidden_Encoding\")(\n",
        "    batch_normalize1\n",
        ")\n",
        "batch_normalize2 = BatchNormalization()(hidden_layer)\n",
        "# create the output layer of the encoder\n",
        "z = Dense(latent_dimension, name=\"Mean\")(batch_normalize2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOYl_ntDShyq"
      },
      "outputs": [],
      "source": [
        "from keras import Model\n",
        "\n",
        "\n",
        "encoder = Model(input_encoder, z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntfFwElrS2Y1"
      },
      "outputs": [],
      "source": [
        "# create an input layer of the decoder\n",
        "input_decoder = Input(shape=(latent_dimension,), name=\"Input_Decoder\")\n",
        "batch_normalize1 = BatchNormalization()(input_decoder)\n",
        "# create a hidden layer\n",
        "decoder_hidden_layer = Dense(hidden_nodes, activation=\"relu\", name=\"Hidden_Decoding\")(\n",
        "    batch_normalize1\n",
        ")\n",
        "batch_normalize2 = BatchNormalization()(decoder_hidden_layer)\n",
        "# create the output layer of the autoencoder\n",
        "decoded = Dense(123, activation=\"linear\", name=\"Decoded\")(batch_normalize2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4O7Ndu1S48H"
      },
      "outputs": [],
      "source": [
        "# specify the input and output layer of the decoder\n",
        "decoder = Model(input_decoder, decoded, name=\"Decoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiJVBTcyS8cB"
      },
      "outputs": [],
      "source": [
        "# create a complete autoencoder architecture\n",
        "encoder_decoder = decoder(encoder(input_encoder))\n",
        "\n",
        "ae = Model(input_encoder, encoder_decoder)\n",
        "# a summary of the autoencoder model\n",
        "ae.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.fillna(-1, inplace=True)"
      ],
      "metadata": {
        "id": "EkYKFdMmarmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vIV8KlRS-az"
      },
      "outputs": [],
      "source": [
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# set the seed for random number generation\n",
        "set_seed(2021)\n",
        "# compile the complete autoencoder model\n",
        "ae.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
        "\n",
        "# use checkpoint during model training\n",
        "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
        "                               verbose=0,\n",
        "                               save_best_only=True)\n",
        "earlystopping = EarlyStopping(monitor='val_loss', patience=1, verbose=0)\n",
        "\n",
        "# train the autoencoder model on the input data\n",
        "history = ae.fit(\n",
        "    data, data, shuffle=True, epochs=10, batch_size=20, validation_split=0.2, verbose=0\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "ae = load_model('model.h5')"
      ],
      "metadata": {
        "id": "xQee7qObn9TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFmMcu5LTG72"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "sns.set(font_scale=2)\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "\n",
        "def model_analysis(history):\n",
        "    # extract the training loss and validation loss values from the history object\n",
        "    train_loss = history[\"loss\"]\n",
        "    val_loss = history[\"val_loss\"]\n",
        "    # x-axis values for the plot\n",
        "    t = np.linspace(1, len(train_loss), len(train_loss))\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "    plt.title(\"Mean squared error\")\n",
        "    # plot the training loss and validation loss against the epoch values on the x-axis\n",
        "    sns.lineplot(x=t, y=train_loss, label=\"Train\", linewidth=3)\n",
        "    sns.lineplot(x=t, y=val_loss, label=\"Validation\", linewidth=3)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.savefig(\"FirstNet.png\", dpi=400)\n",
        "    plt.show()\n",
        "    # the square root of the final training loss and validation loss values\n",
        "    print(f\"Training MSE = {np.sqrt(train_loss[-1])}\")\n",
        "    print(f\"Validation MSE = {np.sqrt(val_loss[-1])}\")\n",
        "\n",
        "\n",
        "model_analysis(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFNcf1zCiNP5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(\"Empirical distribution function z\")\n",
        "plt.xticks((-8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14))\n",
        "# use the encoder model to obtain the latent representation (z) of the data input\n",
        "plt.hist(encoder.predict(data), bins=30, density=True)\n",
        "plt.savefig(\"DistInternal.png\", dpi=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP8jmpGiiWkP"
      },
      "outputs": [],
      "source": [
        "ae.predict(data)[0,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuDkaaY6jtMm"
      },
      "outputs": [],
      "source": [
        "# plot the empirical distribution function for the values obtained from the encoder's predictions on the dataset\n",
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "\n",
        "\n",
        "ecdf = ECDF(encoder.predict(data)[:, 0])\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.title(\"Empirical distribution function z\")\n",
        "x = (-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)\n",
        "plt.yticks(np.linspace(0, 1, 11))\n",
        "plt.xticks(x)\n",
        "plt.grid()\n",
        "plt.plot(x, ecdf(x), linewidth=3)\n",
        "plt.savefig(\"EmpiricalDF.png\", dpi=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-bb5VdbrKNb"
      },
      "outputs": [],
      "source": [
        "from scipy.interpolate import interp1d\n",
        "\n",
        "\n",
        "x = (-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)\n",
        "# calculate the sample EDF values at the specified x-values\n",
        "sample_edf_values_at_slope_changes = [ecdf(i) for i in x]\n",
        "inverted_edf = interp1d(sample_edf_values_at_slope_changes, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI-p2VDnrRib"
      },
      "outputs": [],
      "source": [
        "from numpy.random import uniform\n",
        "from numpy.random import seed\n",
        "\n",
        "# number of data points to generate\n",
        "N = 10000\n",
        "seed(2021)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.title(\"Inverted empirical distribution function\")\n",
        "x = np.linspace(0.30, 0.80, 80)\n",
        "plt.xticks(np.linspace(0, 1.0, 11))\n",
        "plt.grid()\n",
        "plt.plot(x, inverted_edf(x), linewidth=3)\n",
        "plt.savefig(\"InvertedEmpiricalDF.png\", dpi=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_UbhP10rVJ1"
      },
      "outputs": [],
      "source": [
        "# number of data points to generate\n",
        "N = 10000\n",
        "seed(2021)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.title(\"Empirical distribution function z\")\n",
        "plt.xticks((-5, -4, -3, -2, -1, 0, 1, 2, 3, 4))\n",
        "# N random values from a uniform distribution are transformed to follow a specific distribution using the inverted EDF\n",
        "plt.hist(inverted_edf(uniform(0.30, 0.80, N)), bins=30, density=True)\n",
        "plt.savefig(\"DistGenerated.png\", dpi=400)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "# reconstruct the original data\n",
        "normal_reconstructions = ae.predict(data)\n",
        "# compute the Mean Absolute Error between the reconstructed data and the original data\n",
        "normal_loss = tf.losses.mae(normal_reconstructions, data)\n",
        "plt.hist(normal_loss, bins=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P4b5cOqTafyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = np.mean(normal_loss) + 2*np.std(normal_loss)\n",
        "print(threshold)"
      ],
      "metadata": {
        "id": "zDj2gmAybTEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 8))\n",
        "plt.hist(normal_loss, bins=10, color='b', label=\"normal loss\")\n",
        "# add a vertical line to the plot at the position of the threshold value\n",
        "plt.axvline(threshold, color='r', label=\"threshold\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8J_bBLRpbWKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the indices that would sort the mae array in descending order\n",
        "sorted_indices = np.argsort(normal_loss)[::-1]"
      ],
      "metadata": {
        "id": "1EJ9vFLIorUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 100  # Number of samples to select\n",
        "samples_with_high_error = data.iloc[sorted_indices[:k]]\n",
        "samples_with_high_error.head()"
      ],
      "metadata": {
        "id": "pzkZLpOWovDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find indices of tuples with reconstruction values larger than the threshold\n",
        "anomaly_indices = np.where(normal_loss > threshold)[0]\n",
        "\n",
        "# Select the corresponding tuples from the original data\n",
        "anomaly_tuples = data.iloc[anomaly_indices]\n",
        "\n",
        "# Print the anomaly tuples\n",
        "anomaly_tuples.head()\n"
      ],
      "metadata": {
        "id": "m5mb9wZso06U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_tuples.shape"
      ],
      "metadata": {
        "id": "whzq8OvKsOPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbkf9IEWuO6f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}